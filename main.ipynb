{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mss import mss\n",
    "import pydirectinput \n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import Env\n",
    "from gym.spaces import Box, Discrete\n",
    "import re\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = \"C:/Program Files/Tesseract-OCR/tesseract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebGame(Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Step Spaces\n",
    "        self.observation_space = Box(0, 255, shape= (1,83,100), dtype=np.uint8)\n",
    "        self.action_space = Discrete(4)\n",
    "\n",
    "        # Define Extraction params\n",
    "        self.cap = mss()\n",
    "        self.game_location = {\"top\": 280, \"left\": 200, \"width\": 540, \"height\": 530}\n",
    "        self.done_location = {\"top\": 470, \"left\": 260, \"width\": 420, \"height\": 70}\n",
    "        self.score_location = {\"top\": 130, \"left\": 518, \"width\": 80, \"height\": 30}\n",
    "    \n",
    "    def step(self, action):\n",
    "        action_map = {\n",
    "            0: \"up\",\n",
    "            1: \"down\",\n",
    "            2: \"left\",\n",
    "            3: \"right\"\n",
    "        }\n",
    "\n",
    "        if action:\n",
    "            pydirectinput.press(action_map[action])\n",
    "\n",
    "        done = self.get_done()\n",
    "        new_observation = self.get_observation()\n",
    "\n",
    "        reward = 1\n",
    "\n",
    "        info = {}\n",
    "\n",
    "        return new_observation, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        cv2.imshow(\"Game\", np.array(self.cap.grab(self.game_location))[:, : , :3])\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            self.close()\n",
    "\n",
    "    def reset(self):\n",
    "        time.sleep(0.5)\n",
    "        pydirectinput.click(650,230)\n",
    "        time.sleep(0.5)\n",
    "        return self.get_observation()\n",
    "    \n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def get_observation(self):\n",
    "        raw = np.array(self.cap.grab(self.game_location))[:,:,:3]\n",
    "        gray = cv2.cvtColor(raw ,cv2.COLOR_BGR2GRAY)\n",
    "        resized = cv2.resize(gray, (100, 83))\n",
    "        channel= np.reshape(resized, (1,83, 100))\n",
    "\n",
    "        return raw\n",
    "    \n",
    "    def get_done(self):\n",
    "        done_cap = np.array(self.cap.grab(self.done_location))[:,:,:3]\n",
    "\n",
    "        done_strings = [\"Game\", \"Gahe\", \"Gaam\"]\n",
    "\n",
    "        done = False\n",
    "\n",
    "        res = pytesseract.image_to_string(done_cap)[:4]\n",
    "\n",
    "        if res in done_strings:\n",
    "            done = True\n",
    "\n",
    "        return done\n",
    "    \n",
    "    # def get_score(self):\n",
    "    #     score_cap = np.array(self.cap.grab(self.score_location))[:,:,:3]\n",
    "    #     score = (pytesseract.image_to_string(score_cap))\n",
    "    #     match = re.search(r'\\d+', score)\n",
    "    #     if match:\n",
    "    #         int_score = int(match.group())\n",
    "    #     else:\n",
    "    #         int_score = score\n",
    "\n",
    "    #     return int_score, score_cap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WebGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing loop for 10 Episodes\n",
    "\n",
    "# episodes = 1\n",
    "\n",
    "# for episode in range(1, episodes + 1):\n",
    "#     obs = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "\n",
    "#     while not done:\n",
    "#         obs, reward, done, info = env.step(env.action_space.sample())\n",
    "#         total_reward += reward\n",
    "\n",
    "#     print(f\"Total reward for episode {episode} is {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback\n",
    "\n",
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker\n",
    "\n",
    "env_checker.check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving models (callback)\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    \n",
    "    def __init__(self, check_freq, save_path, verbose = 1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok = True)\n",
    "            \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, \"best_model_{}\".format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "        \n",
    "        return True\n",
    "    \n",
    "CHECKPOINT_DIR = \"./train/\"\n",
    "LOG_DIR = \"./logs/\"\n",
    "callback = TrainAndLoggingCallback(check_freq=1000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Build DQN\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "model = DQN(\"CnnPolicy\", env = env, tensorboard_log=LOG_DIR, verbose=1, buffer_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000, callback=callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
